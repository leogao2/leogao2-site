<!DOCTYPE html>
<html lang="en-us">

        <!--
            What brings you to these parts, fellow adventurer?
        -->


<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-146499048-1', 'auto');
ga('send', 'pageview');

</script>

<script type="text/javascript">
    function timer11(){ga('send', 'event', 'TimeOnPage', '1', '11-30 seconds', { 'nonInteraction': 1 });}
    function timer31(){ga('send', 'event', 'TimeOnPage', '2', '31-60 seconds', { 'nonInteraction': 1 });}
    function timer61(){ga('send', 'event', 'TimeOnPage', '3', '61-180 seconds', { 'nonInteraction': 1 });}
    function timer181(){ga('send', 'event', 'TimeOnPage', '4', '181-600 seconds', { 'nonInteraction': 1 });}
    function timer601(){ga('send', 'event', 'TimeOnPage', '5', '601-1800 seconds', { 'nonInteraction': 1 });}
    function timer1801(){ga('send', 'event', 'TimeOnPage', '6', '1801+ seconds', { 'nonInteraction': 1 });}
    ga('send', 'event', 'TimeOnPage', '0', '0-10 seconds', { 'nonInteraction': 1 });
    setTimeout(timer11,11000);
    setTimeout(timer31,31000);
    setTimeout(timer61,61000);
    setTimeout(timer181,181000);
    setTimeout(timer601,601000);
    setTimeout(timer1801,1801000);
</script>

<!-- End Google Analytics -->


  
  <title>Building AGI Using Language Models | Leo Gao</title>
  <script>
    var Hyphenopoly = {
        require: {
            "en-us": "Supercalifragilisticexpialidocious"
        },
        setup: {
            selectors: {
                ".article-entry": {},
                ".main-info": {}
            }
        }
    };
    </script>   
    <script src="/js/Hyphenopoly_Loader.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="This post is a draft. Despite the buzz around GPT-3, it is, in and of itself, not AGI. In many ways, this makes it similar to AlphaGo or Deep Blue; while approaching human abiliy in one domain (playi">
<meta name="keywords" content="Language Models,GPT-3,Alignment,AGI">
<meta property="og:type" content="article">
<meta property="og:title" content="Building AGI Using Language Models">
<meta property="og:url" content="https://leogao.dev/2020/08/09/Building-AGI-Using-Language-Models/index.html">
<meta property="og:site_name" content="Leo Gao">
<meta property="og:description" content="This post is a draft. Despite the buzz around GPT-3, it is, in and of itself, not AGI. In many ways, this makes it similar to AlphaGo or Deep Blue; while approaching human abiliy in one domain (playi">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://leogao.dev/images/agi-lms/sunset1-glitch.png">
<meta property="og:image" content="https://leogao.dev/images/gpt3/perf_scaling_compute.png">
<meta property="og:image" content="https://leogao.dev/images/agi-lms/mesa.png">
<meta property="og:image" content="https://leogao.dev/images/agi-lms/mcts.png">
<meta property="og:image" content="https://leogao.dev/images/agi-lms/paperclipsamzn.png">
<meta property="og:updated_time" content="2020-08-17T00:50:53.030Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Building AGI Using Language Models">
<meta name="twitter:description" content="This post is a draft. Despite the buzz around GPT-3, it is, in and of itself, not AGI. In many ways, this makes it similar to AlphaGo or Deep Blue; while approaching human abiliy in one domain (playi">
<meta name="twitter:image" content="https://leogao.dev/images/agi-lms/sunset1-glitch.png">
<meta name="twitter:creator" content="@nabla_theta">
  
    <link rel="alternate" href="/atom.xml" title="Leo Gao" type="application/atom+xml">
  
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous"><!-- hexo-inject:begin --><link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css'><!-- hexo-inject:end -->
</head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo"></a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="https://twitter.com/nabla_theta">Twitter</a>
        
          <a class="main-nav-link" href="https://github.com/leogao2">Github</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <!--a id="nav-search-btn" class="nav-icon" title="Search"></a-->
      </nav>
      <!--div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://leogao.dev"></form>
      </div-->
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Building-AGI-Using-Language-Models" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <div class="article-date">
  Created: <time datetime="2020-08-09T06:00:00.000Z" itemprop="datePublished">2020-08-09</time>&emsp;
  Modified: <time datetime="2020-08-09T06:00:00.000Z" itemprop="datePublished">2020-08-09</time>
</div>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Building AGI Using Language Models
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- TOC -->
        
        <p><img src="/images/agi-lms/sunset1-glitch.png" alt></p>
<p><em><strong>This post is a draft.</strong></em></p>
<p><span class="smallcaps">Despite the buzz around GPT-3, it is, in and of itself, not AGI.</span> In many ways, this makes it similar to AlphaGo or Deep Blue; while approaching human abiliy in one domain (playing Chess/Go, or writing <em>really</em> impressively), it doesn’t really seem like it will do <span class="smallcaps"><a href="https://wiki.lesswrong.com/wiki/Intelligence_explosion" target="_blank" rel="noopener">Scary AGI Things™</a></span> any more than AlphaGo is going to be turning the Earth into paperclips anytime soon. While its writings are impressive at emulating humans, GPT-3 (or any potential future GPT-x) has no memory of past interactions, nor is it able to follow goals or maximize utility. However, language modelling has one <em>crucial</em> difference from Chess or Go or image classification. Natural language essentially encodes information about the world—the <em>entire</em> world, not just the world of the Goban, in a much more expressive way than any other modality ever could.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> By harnessing the world model embedded in the language model, it may be possible to build a proto-AGI.</p>
<h1>World Modelling</h1>
<p>The explicit goal of a language model is only to <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" target="_blank" rel="noopener">maximize likelihood</a> of the model on natural language data. In the <a href="https://leogao.dev/2019/10/27/The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models/">autoregressive formulation</a> that GPT-3 uses, this means being able to predict the next word as well as possible. However, this objective places much more weight on large, text-scale differences like grammar and spelling than fine, subtle differences in semantic meaning and logical coherency, which reflect as very subtle shifts in distribution. Once the former are near-perfect, though, the only place left to keep improving is the latter.</p>
<p>At the extreme, any model whose loss reaches the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" target="_blank" rel="noopener">Shannon entropy</a> of <a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/1999-00/information-theory/entropy_of_english_9.html" target="_blank" rel="noopener">natural language</a>—the theoretical lowest loss a language model can possibly achieve, due to the inherent randomness of language—will be <em>completely indistinguishable</em> from writings by a real human in every way, and the closer we get to it, the more abstract the effect on quality of each bit of improvement in loss. Or, said differently, stringing words together using <a href="https://github.com/jsvine/markovify#markovify-in-the-wild" target="_blank" rel="noopener">Markov chain generators</a> gets you 50% of the way there, figuring out grammar gets you another 50% of the remaining distance, staying on topic across paragraphs gets you another 50% of the remaining distance, being logically consistent gets you another 50% of the remaining distance, and so on.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtable><mtr><mtd><mrow><mi>H</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>=</mo><mo>−</mo><mi mathvariant="double-struck">E</mi><mo>[</mo><mi>log</mi><mi mathvariant="double-struck">P</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>]</mo><mo>=</mo><mo>−</mo><msub><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi mathvariant="normal">Ω</mi></mrow></msub><msub><mi>f</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mi>log</mi><msub><mi>f</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{aligned}
H(X) = -\mathbb E[\log \mathbb P(X)] = -\sum_{x \in \Omega} f_X(x) \log f_X(x)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.4358555000000004em;"></span><span class="strut bottom" style="height:2.3717110000000003em;vertical-align:-0.9358555em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist"><span style="top:-0.3858505000000001em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord">−</span><span class="mord mathbb">E</span><span class="mopen">[</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathbb">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mclose">]</span><span class="mrel">=</span><span class="mord">−</span><span class="mop op-limits"><span class="vlist"><span style="top:1.194336em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">x</span><span class="mrel">∈</span><span class="mord mathrm">Ω</span></span></span></span><span style="top:-0.000005000000000032756em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.10764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></span></span>
<p><span class="caption">Shannon Entropy: the number of bits necessary, on average to specify one piece of text.</span></p>
<p>Why? Because if you have a coherent-but-not-logically consistent model, becoming more logically consistent helps you predict language better. Having a model of human behavior helps you predict language better. Having a model of the world helps you predict language better. As the low-hanging fruits of grammar and basic logical coherence are taken, the only place left for the model to keep improving the loss is a world model. Predicting text is <a href="http://mattmahoney.net/dc/rationale.html" target="_blank" rel="noopener">equivalent to AI</a>.</p>
<p>The thing about GPT-3 that <a href="https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/">makes it so important</a> is that it provides evidence that as long as we keep increasing the model size, we can keep driving down the loss, <a href="https://arxiv.org/abs/2001.08361" target="_blank" rel="noopener">possibly right up until</a> it hits the Shannon entropy of text. No need for clever architectures or complex handcrafted heuristics. Just by scaling it up we can get a better language model, and a better language model entails a better world model.</p>
<p>But how do we use this language model if it’s buried implicitly-represented inside GPT-x, though? Well, we can literally just <em>ask</em> it, in natural language, what it thinks will happen next given a sequence of events, and its output distribution will approximate the distribution of what the average human thinks would happen next after those events. Great—we’ve got ourselves a usable world model.</p>
<p>“But wait!” you say, “<a href="https://aiweirdness.com/post/621186154843324416/all-your-questions-answered" target="_blank" rel="noopener">Various</a> <a href="https://twitter.com/eigenrobot/status/1284042570507542528" target="_blank" rel="noopener">experiments</a> have shown that GPT-3 often fails at world modelling, and just conjecturing that adding more parameters will fix the problem is a <em>massive</em> leap!” If you’re thinking this, you’re absolutely correct. The biggest and most likely to be wrong assumption that I’m making is that larger models will develop better world models. Since as loss approaches the Shannon entropy its world modelling ability has to become about as good than the average human on the internet<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>, this boils down to two questions: “Will we really make models whose loss gets close enough to the Shannon entropy?” and “How close is close enough, in order to have the world modelling capabilities to make all this practical?”</p>
<p><img src="/images/gpt3/perf_scaling_compute.png" alt="Loss keeps going down with more parameters and compute. (&lt;a href='https://arxiv.org/abs/2005.14165'&gt;Source&lt;/a&gt;)"></p>
<p>The answer to the first question is “most likely”—that’s the <a href="https://leogao.dev/2020/05/29/GPT-3-A-Brief-Summary/">main takeaway of GPT-3</a>. The answer to the second question is… nobody knows. Some have demonstrated <a href="https://news.ycombinator.com/item?id=23990902" target="_blank" rel="noopener">ways of making GPT-3 better at world modelling</a>, but this alone is probably not sufficient. When models with 1 trillion, then 10 trillion, then 100 trillion parameters become available, we will have empirical evidence to see whether this assumption is correct. If GPT-x demonstrates uncanny ability to predict outcomes in the real world, then this just might work.</p>
<h1>Putting the pieces together</h1>
<p>A world model alone does not an agent make, though.<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> So what does it take to make a world model into an agent? Well, first off we need a goal, such as <a href="https://wiki.lesswrong.com/wiki/Paperclip_maximizer" target="_blank" rel="noopener">“maximize number of paperclips”</a>. Then, we just ask the world model “What action can I take to maximize the number of paperclips I have?” Simple, right? Actually, not quite. The problem is that our world model probably won’t be able to consider all the possible things that could happen next well enough to make a reasonable answer.</p>
<p><img src="/images/agi-lms/mesa.png" alt="GPT-3 considers mesa-optimization. (Source: OpenAI API)"></p>
<p>So what can we do instead? Well, asking the world model for a list of things you could do in a given world state would probably not be outside the capabilities of a sufficiently powerful language model (think: “I am in situation <span class="smallcaps">xyz</span>. Here is a list of things I could do:”). Similarly, asking the world model how much reward you’d get in some hypothetical world where you took a sequence of actions would probably be possible too—imagine asking something along the lines of “I go to ebay. I look up paperclips, sorted by price ascending. I spend $100 on the first item on the list. How many paperclips will I have?”<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> This will let us figure out what actions the agent can take in any given step (policy function), as well as how much reward each sequence of steps will net the agent (value function).</p>
<p>So now, to estimate the state-action value of any action, we can simply do <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" target="_blank" rel="noopener">Monte Carlo Tree Search</a> to estimate the state-action values! Starting from a given agent state, we can roll out sequences of actions using the world model. By integrating over all rollouts, we can know how much future expected reward the agent can expect to get for each action it considers. Then, we can simply use, for example, a greedy policy with that state-action value function, to decide on actions to take.</p>
<p><img src="/images/agi-lms/mcts.png" alt="Monte Carlo Tree Search visualized (&lt;a href='https://www.researchgate.net/figure/Phases-of-the-Monte-Carlo-tree-search-algorithm-A-search-tree-rooted-at-the-current_fig1_312172859'&gt;Source&lt;/a&gt;)"></p>
<p>Each of these actions is likely to be very high level, such as “figure out the cheapest way to buy paperclips”, but thanks to the flexibility of language we can describe very complex ideas with short sequences of tokens. To actually execute these abstract actions once the agent decides on an action, that action can be broken down using the language model into smaller sub-goals such as “figure out the cheapest paperclips on Amazon”, similar to <a href="https://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf" target="_blank" rel="noopener">Hierarchical Reinforcement Learning</a>. Possibly even just breaking actions down into a detailed list of instructions would be feasible, depending on the capabilities of the model and how abstract the actions are.</p>
<p>We can represent the agent state as natural language, too. Since the agent state is just a compressed representation of the observations, we can ask the language model to summarize the important information of any observations for its own internal world state. The language model could be used to periodically prune (i.e forget) the information inside its state, too, to make room for more observations.</p>
<p>Altogether, this gets us a system where we can pass observations from the outside world in, spend some time thinking about what to do, and output an action in natural language.</p>
<p>To handle input, you could have an input module that turns various modalities of observations into summarized text with respect to the current agent state. For instance, you could use something like <a href="https://openai.com/blog/image-gpt/" target="_blank" rel="noopener">iGPT</a> to input camera images or screenshots, or raw HTML from webpages that the agent requests. How exactly this is done is tangential to the point; all that matters is that somehow the inputs are all converted to text and added to the agent state. The examples I have provided are just to convince you that it’s absolutely not insurmountable.</p>
<p>Finally, to get the model to actually act in the world, you could again use the language model to translate natural language into <a href="https://www.pscp.tv/Microsoft/1OyKAYWPRrWKb?t=29m19s" target="_blank" rel="noopener">code that is then executed</a>, or <a href="https://vimeo.com/427943407/98fe5258a7" target="_blank" rel="noopener">shell commands</a>, or sequences of keypresses, or any of a number of other possible ways. Like input, there are an infinitude of different ways to solve the output problem, and which one turns out to be the best is entirely irrelevant to our discussion; all that matters is that it’s possible to get various modalities in and out of the text-only agent.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></p>
<p><img src="/images/agi-lms/paperclipsamzn.png" alt="An example of an input module taking a screenshot input combined with the current agent state to give an observation with the information needed by the agent."></p>
<h1>Conclusion</h1>
<p>This is more a thought experiment than something that’s actually going to happen tomorrow; GPT-3 today just isn’t good enough at world modelling. Also, this method depends heavily on at least one major assumption—that bigger future models will have much better world modelling capabilities—and a bunch of other smaller implicit assumptions. <strong><span class="smallcaps">However</span></strong>, this might be the closest thing we ever get to a chance to <a href="https://intelligence.org/2017/10/13/fire-alarm/" target="_blank" rel="noopener">sound the fire alarm</a> for AGI: there’s now a concrete path to proto-AGI that has a non-negligable chance of working.</p>
<p><em>Thanks to <a href="https://twitter.com/zitterbewegung" target="_blank" rel="noopener">zitterbewegung</a>, <a href="https://twitter.com/realmeatyhuman" target="_blank" rel="noopener">realmeatyhuman</a>, and <a href="https://twitter.com/theshawwn" target="_blank" rel="noopener">Shawn Presser</a> for taking the time to provide feedback on the draft of this blog post!</em></p>
<!--[^toolai]: One common line of argument is that such "tool AIs" are good enough and have desirable safety properties, and therefore agentistic AI need not ever be developed. One argument against this idea is that [agent AI will inevitably be more economically valuable than tool AI](https://www.gwern.net/Tool-AI), which would encourage their development.-->
<!--

Going from a model that just strings uniformly chosen words together to a simple [Markov chain generator](https://github.com/jsvine/markovify#markovify-in-the-wild) makes a *massive* difference, but the loss difference between a Markov chain generator and GPT-2 is much smaller despite GPT-2 being able to manage grammatically correct sentences nearly all of the time, and the loss difference between GPT-2 and GPT-3 is smaller still, despite massive subjective improvements in quality. 

The difference between "the quick brown fox jumps over the lazy dog" and "shxixpgozcvoomz jcxfgabsdcxfrevogjsqewdnmkj" is *massive*, but the difference between "the quick brown fox jumps over the lazy dog" and "the quick brown fox jumps over the lazy dog"

Going from a model that just strings uniformly chosen words together to a model that uses words in the same [Zipf distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) as regular text earns a large decrease in loss, because if you always pick common words like "the" you have a much better chance of being right than picking rare words. Another big decrease comes from considering n-gram frequencies rather than just single-word frequencies; this is the driving idea behind [Markov chain generators](https://github.com/jsvine/markovify#markovify-in-the-wild), which generate surprisingly good text considering how just how simple they are. Now, these 

-->
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Images aren’t nearly as good as text for encoding unambiguous, complex ideas, unless you put text <em>in</em> images, but at that point that’s just language modelling with extra steps. Also, images <em>can</em> encode complex ideas, but in a much less information-dense manner; I have no doubt that a sufficiently large image model could also learn such information about the world through images, but most likely at multiple orders of magnitude higher cost than an equivalent-world-modelling-capability language model. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Another way to look at this is at cherrypicking. Most impressive demos of GPT-3 where it displays impressive knowledge of the world <em>are</em> cherrypicked, but what that tells us is that the model needs to improve by approx <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mi>N</mi><mo>)</mo><mi mathvariant="normal">/</mi><mi>L</mi></mrow><annotation encoding="application/x-tex">\log_2(N)/L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="vlist"><span style="top:0.24444em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mclose">)</span><span class="mord mathrm">/</span><span class="mord mathit">L</span></span></span></span> bits, where N and L are the number of cherrypickings necessary and the length of the generations in consideration, respectively, to reach that level of quality. In other words, cherrypicking provides a window into how good future models could be; and typically, cherrypicked samples are much more logically coherent.</p>
<blockquote>
<p>A Markov chain text generator trained on a small corpus represents a huge leap over randomness: instead of having to generate countless quadrillions of samples, one might only have to generate millions of samples to get a few coherent pages; this can be improved to hundreds or tens of thousands by increasing the depth of the n of its n-grams. […] But for GPT-3, once the prompt is dialed in, the ratio appears to have dropped to closer to 1:5—maybe even as low as 1:3! <cite><a href="https://www.gwern.net/GPT-3#quality" target="_blank" rel="noopener">gwern</a></cite></p>
</blockquote>
 <a href="#fnref2" class="footnote-backref">↩︎</a></li>
<li id="fn3" class="footnote-item"><p>Which, despite what the all-too-common snide remarks about the intelligence of the average internet user would have you believe, is actually not that bad! <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>A pure world model is in a lot of ways similar to the idea of <a href="https://www.lesswrong.com/posts/XddMs9kSGtm6L8522/a-taxonomy-of-oracle-ais" target="_blank" rel="noopener">Oracle AIs</a>, specifically Predictors. Whether these LM-based world models will be powerful enough to model the impact of their own outputs is yet to be seen. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>A more involved way we could do this is by finetuning the model, or <a href="https://arxiv.org/abs/1912.02164" target="_blank" rel="noopener">steering</a> it using a smaller model, etc. to get the model to output the kinds of things we need, if just asking nicely and providing examples, like, how GPT-3 is used today, turns out to not be good enough. <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Given a <a href="https://yudkowsky.net/singularity/aibox/" target="_blank" rel="noopener">strong enough agent</a>, it might not even be necessary to give it the ability to actually act in the real world. LM-based agents <em>probably</em> (hopefully?) won’t get this strong, though. <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://leogao.dev/2020/08/09/Building-AGI-Using-Language-Models/" data-id="ckdxsuyxv0002vpfm9kfhm67v" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AGI/">AGI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Alignment/">Alignment</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPT-3/">GPT-3</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Language-Models/">Language Models</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/05/29/GPT-3-A-Brief-Summary/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Why GPT-3 Matters</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside class="sidebar sidebar-left">
    <div class="">
        <!--div class="mainnav">
            <p><a href='/' class="mainnav-link">Home</a></p>
            
        </div-->
    </div>
    <div class="leftfnbegin"></div>
    <div class="sidebar-left-footnotes"></div>
</aside>
<aside class="sidebar sidebar-right">
    
      
  <div class="widget-wrap sidebar-align">
    <h3 class="widget-title">About Me</h3>
    <ul class="about-me">
      
      <li><div class="avatar"><img class="ava-img" title="About me" src="/images/profile_small.png" /></div></li>
      
      <div class="authname">Leo Gao</div>
      <div class="sdesc"></div>

      <div class="contact">
          <div class="contact-item"><a href="https://twitter.com/nabla_theta"><i class="fa fa-twitter" aria-hidden="true"></i> @nabla_theta</a></div>
          <div class="contact-item"><a href="https://github.com/leogao2"><i class="fa fa-github" aria-hidden="true"></i> leogao2</a></div>
          <div class="contact-item"><a href="mailto:leogao31@gmail.com"><i class="fa fa-envelope" aria-hidden="true"></i> leogao31@gmail.com</a></div>
      </div>
    </ul>


  </div>
  
    
      
  <div class="widget-wrap sidebar-align">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget recentposts">
      <ul>
        
          <li>
            <a href="/2020/05/29/GPT-3-A-Brief-Summary/">Why GPT-3 Matters</a>
          </li>
        
          <li>
            <a href="/2020/04/01/Breaking-Down-Reddit-s-Imposter/">Breaking Down Reddit&#39;s Imposter</a>
          </li>
        
          <li>
            <a href="/2020/01/31/Multidisk-Filesystems-A-Comparison/">Multidisk Filesystems: A Comparison</a>
          </li>
        
          <li>
            <a href="/2019/12/31/The-Decade-of-Deep-Learning/">The Decade of Deep Learning</a>
          </li>
        
          <li>
            <a href="/2019/11/09/Converting-HuggingFace-GPT2-Models-to-Tensorflow-1/">Converting HuggingFace GPT-2 Models to Tensorflow 1.x</a>
          </li>
        
      </ul>
    </div>
  </div>

    
    <div class="rightfnbegin"></div>
    <div class="sidebar-right-footnotes"></div>

</aside>

<div class="mobile-fn-float-wrapper">
    <div class="mobile-fn-float">
        <div class="footnote-float-numeral footnote-float-numeral-mobile"></div>
        <div class="footnote-float-content"></div>
        <div class="footnote-float-overflow">...</div>
    </div>
</div>
        
      </div>
      <!--
<footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Leo Gao<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
-->
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="https://twitter.com/nabla_theta" class="mobile-nav-link">Twitter</a>
  
    <a href="https://github.com/leogao2" class="mobile-nav-link">Github</a>
  
</nav>
    

<script src="/js/jquery.min.js"></script>
<script src="/js/footnotes.js"></script>
<script src="https://unpkg.com/@popperjs/core@2"></script>
<script src="https://unpkg.com/tippy.js@6"></script>
<script src="/js/acronym-fancifier.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>