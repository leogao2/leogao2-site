<!DOCTYPE html>
<html lang="en-us">

        <!--
            What brings you to these parts, fellow adventurer?
        -->


<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-146499048-1', 'auto');
ga('send', 'pageview');

</script>

<script type="text/javascript">
    function timer11(){ga('send', 'event', 'TimeOnPage', '1', '11-30 seconds', { 'nonInteraction': 1 });}
    function timer31(){ga('send', 'event', 'TimeOnPage', '2', '31-60 seconds', { 'nonInteraction': 1 });}
    function timer61(){ga('send', 'event', 'TimeOnPage', '3', '61-180 seconds', { 'nonInteraction': 1 });}
    function timer181(){ga('send', 'event', 'TimeOnPage', '4', '181-600 seconds', { 'nonInteraction': 1 });}
    function timer601(){ga('send', 'event', 'TimeOnPage', '5', '601-1800 seconds', { 'nonInteraction': 1 });}
    function timer1801(){ga('send', 'event', 'TimeOnPage', '6', '1801+ seconds', { 'nonInteraction': 1 });}
    ga('send', 'event', 'TimeOnPage', '0', '0-10 seconds', { 'nonInteraction': 1 });
    setTimeout(timer11,11000);
    setTimeout(timer31,31000);
    setTimeout(timer61,61000);
    setTimeout(timer181,181000);
    setTimeout(timer601,601000);
    setTimeout(timer1801,1801000);
</script>

<!-- End Google Analytics -->


  
  <title>The Difficulties of Text Generation using Autoregressive Language Models: A Brief Overview | Leo Gao</title>
  <script>
    var Hyphenopoly = {
        require: {
            "en-us": "Supercalifragilisticexpialidocious"
        },
        setup: {
            selectors: {
                ".article-entry": {},
                ".main-info": {}
            }
        }
    };
    </script>   
    <script src="/js/Hyphenopoly_Loader.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Interest in text-generating models has been rekindled in the past year—in large part due to GPT-2, which primarily demonstrates the effectiveness of using the Transformer architecture with bigger mod">
<meta name="keywords" content="Machine Learning,Natural Language Processing,Language Models,GPT-2,Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="The Difficulties of Text Generation using Autoregressive Language Models: A Brief Overview">
<meta property="og:url" content="https://leogao.dev/2019/10/27/The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models/index.html">
<meta property="og:site_name" content="Leo Gao">
<meta property="og:description" content="Interest in text-generating models has been rekindled in the past year—in large part due to GPT-2, which primarily demonstrates the effectiveness of using the Transformer architecture with bigger mod">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://leogao.dev/images/geb_thumbnail_small.jpg">
<meta property="og:updated_time" content="2020-06-14T20:10:26.987Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Difficulties of Text Generation using Autoregressive Language Models: A Brief Overview">
<meta name="twitter:description" content="Interest in text-generating models has been rekindled in the past year—in large part due to GPT-2, which primarily demonstrates the effectiveness of using the Transformer architecture with bigger mod">
<meta name="twitter:image" content="https://leogao.dev/images/geb_thumbnail_small.jpg">
<meta name="twitter:creator" content="@nabla_theta">
  
    <link rel="alternate" href="/atom.xml" title="Leo Gao" type="application/atom+xml">
  
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192" href="/android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">

  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous"><!-- hexo-inject:begin --><link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css'><!-- hexo-inject:end -->
</head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo"></a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="https://twitter.com/nabla_theta">Twitter</a>
        
          <a class="main-nav-link" href="https://github.com/leogao2">Github</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <!--a id="nav-search-btn" class="nav-icon" title="Search"></a-->
      </nav>
      <!--div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://leogao.dev"></form>
      </div-->
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <div class="article-date">
  Created: <time datetime="2019-10-27T06:00:00.000Z" itemprop="datePublished">2019-10-27</time>&emsp;
  Modified: <time datetime="2020-06-14T06:00:00.000Z" itemprop="datePublished">2020-06-14</time>
</div>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      The Difficulties of Text Generation using Autoregressive Language Models: A Brief Overview
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <!-- TOC -->
        
        <p><img src="/images/geb_thumbnail_small.jpg" alt="Book: Gödel, Escher, Bach"></p>
<p><span class="smallcaps">Interest in text-generating models has been rekindled in the past year</span>—in large part due to <a href="https://openai.com/blog/better-language-models/" target="_blank" rel="noopener">GPT-2</a>, which primarily demonstrates the effectiveness of using the Transformer architecture with <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" target="_blank" rel="noopener">bigger models, bigger data, and bigger compute</a>. Notably, this model achieved <a href="https://paperswithcode.com/sota/language-modelling-on-wikitext-103" target="_blank" rel="noopener">SOTA results on several language modelling datasets</a> without even training on those datasets, showing its impressive generalization capabilities. Following GPT-2, several other entities have also jumped on the bandwagon and released their own large unidirectional language models, such as: <a href="https://rowanzellers.com/grover/" target="_blank" rel="noopener">Grover</a>, <a href="https://nv-adlr.github.io/MegatronLM" target="_blank" rel="noopener">Nvidia’s Megatron-LM</a>, and <a href="https://einstein.ai/presentations/ctrl.pdf" target="_blank" rel="noopener">Salesforce’s CTRL</a>. Setting aside the controversy surrounding OpenAI’s claims that the model is “too dangerous to release,” the text generated by GPT-2 are undeniably far better than previous text generation models. However, these models also exhibit some flaws that may not be fixable purely using the bigger-model paradigm. In this post, we take a quick look at some of these flaws and the attempts to solve them, and discuss some potential directions for future research.</p>
<h2 id="What-is-an-autoregressive-language-model-and-why-does-it-matter">What is an autoregressive language model and why does it matter?</h2>
<p>The core problem of language modelling is approximating the distribution of natural language sequences occurring in English (or Lojban, Navajo, Python, etc) using a parameterized function. To make modelling more manageable, the autoregressive language model formulation factors the ideal language model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p^*(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord">∗</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> into:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtable><mtr><mtd><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo>(</mo><mi>x</mi><mo>)</mo><mo>≈</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msub><mover accent="true"><mrow><mi>p</mi></mrow><mo>^</mo></mover><mi>θ</mi></msub><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{aligned}
p^*(x) \approx \prod_{i=1}^{n} \hat{p}_\theta(x_i | x_{&lt;i})
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.714533em;"></span><span class="strut bottom" style="height:2.929066em;vertical-align:-1.214533em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist"><span style="top:-0.06313599999999986em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord">∗</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">≈</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∏</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathit">p</span></span></span><span style="top:0em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mrel">&lt;</span><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></span></span>
<p>In other words, to make the modelling problem more tractable, we instead train the parameterized function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mrow><mi>p</mi></mrow><mo>^</mo></mover><mi>θ</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\hat{p}_\theta(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit">p</span></span></span><span style="top:0em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> predict the next token conditioned on the previous tokens, and repeat this using the newly generated tokens, appended to the original context, as the new context. We can then obtain an estimate for the likelihood of any given sequence by taking the product across these conditional probabilities. <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>Many problems—including classification and translation—can be equivalently formulated as autoregressive problems or would <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">benefit significantly from a strong pretrained language model</a>. Improving language modelling would also potentially be a major step towards solving <a href="http://mattmahoney.net/dc/rationale.html" target="_blank" rel="noopener">the general AI problem</a>.</p>
<h2 id="Beam-search-and-repetition">Beam search and repetition</h2>
<blockquote>
<p>[…] using likelihood as a decoding objective leads to text that is bland and strangely repetitive.
<cite>Holzman et al. 2019</cite></p>
</blockquote>
<p>In the GPT-2 samples provided, the authors decided to sample with top-k filtering and temperature rather than with beam search, which would be expected to return much higher-quality samples by maximizing likelihood. It was rather surprising, then, when <a href="https://arxiv.org/abs/1904.09751" target="_blank" rel="noopener">“The Curious Case of Neural Text Degeneration” (Holzman et al. 2019)</a> showed that GPT-2 samples with higher predicted likelihood (i.e found via beam search) actually have much lower quality, tending to be extremely repetitive. The authors argue that this modelling problem is due to maximum-likelihood being a fundamentally incorrect sampling objective, and propose nucleus sampling, a sampling method that truncates low-likelihood token predictions (which can lead the model to a “downward spiral”), similar to top-k, while preserving “broad” (tail-heavy) distributions. It could be argued, however, that since sampling a maximum-likelihood sample from the ideal language model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>x</mi></mrow></msub><msup><mi>p</mi><mo>∗</mo></msup><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">argmax_{x} p^*(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord">∗</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> would, by definition, provide the most likely English text, it would <em>already take into account</em> the unlikelihood of extremely bland and repetitive text in English! Thus, the fault lies with the training objective, not the sampling objective.</p>
<p>Another tempting solution is simply to penalize repetition. In fact, shortly following the publication of the Neural Text Degeneration paper, I independently implemented my own GPT-2 beam search sampler; after reproducing the text degeneration issues, I added a simple, arbitrary decoding-time penalty for repeated ngrams, with acceptable results at first glance but little theoretical justification.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> More recently, <a href="https://arxiv.org/abs/1908.04319" target="_blank" rel="noopener">“Neural Text <s>De</s>Generation with Unlikelihood Training” (Welleck, Kulikov et al. 2019)</a> has proposed using a more complex training-time penalization scheme that involves adding a term <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mi>k</mi><msub><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mrow><msup><mi mathvariant="script">C</mi><mi>t</mi></msup></mrow></mrow></msub><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>θ</mi></msub><mo>(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">-k \sum_{c \in \mathcal{C^t}} log(1 - p_\theta(c | x_{&lt;t}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.07738em;vertical-align:-0.32738em;"></span><span class="base textstyle uncramped"><span class="mord">−</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span><span class="mrel">∈</span><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathcal" style="margin-right:0.05834em;">C</span><span class="vlist"><span style="top:-0.289em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">c</span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mrel">&lt;</span><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> to the training objective where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><msup><mi mathvariant="script">C</mi><mi>t</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{C^t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathcal" style="margin-right:0.05834em;">C</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span> is a set of previously used tokens.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> While empirically successful, there is no good theoretical reason why less repetition would better model the underlying distribution.</p>
<h2 id="Exposure-Bias">Exposure Bias</h2>
<blockquote>
<p>[…] the text will usually fall off a quality cliff after a certain point, suddenly becoming strikingly ungrammatical and typo-ridden and full of anomalous paragraph breaks.
<cite>nostalgebraist</cite></p>
</blockquote>
<p>One major problem with maximum-likelihood training of autoregressive models is <a href="https://arxiv.org/pdf/1511.06732.pdf" target="_blank" rel="noopener">exposure bias (Ranzato et al., 2015)</a>. Autoregressive models are only trained and evaluated on samples drawn from the target language distribution, but at evaluation time are fed samples that are themselves generated by the model. This error compounds extremely quickly and it has been observed, though admittedly anecdotally, that GPT-2 exhibits a <a href="https://nostalgebraist.tumblr.com/post/187579086034/it-seems-pretty-clear-to-me-by-now-that-GPT2-is" target="_blank" rel="noopener">sharp drop-off in quality</a> after a certain number of steps.</p>
<h2 id="Future-Work">Future Work</h2>
<p>This problem bears striking resemblance to many problems in reinforcement learning; indeed, existing works such as <a href="https://arxiv.org/abs/1609.05473" target="_blank" rel="noopener">“SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient” (Yu et al., 2016)</a>, <a href="https://arxiv.org/abs/1808.05599" target="_blank" rel="noopener">“Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation” (Tuan et al., 2018)</a>, and <a href="https://arxiv.org/abs/1804.11258" target="_blank" rel="noopener">“Toward Diverse Text Generation with Inverse Reinforcement Learning” (Shi et al., 2018)</a> (this is not intended to be an exhaustive list by any means) use RL for various components of the training pipeline, from propagating the Generator gradient in a GAN setting to using Inverse Reinforcement Learning (which is itself <a href="https://arxiv.org/abs/1611.03852" target="_blank" rel="noopener">deeply connected to GANs</a>).</p>
<p>There is still a long way to go before these reinforcement learning based options become practical for models as large as the ones in GPT-2. An intermediate step is to use existing pretrained language models and tune them in an RL environment. Additionally, an evaluation metric that is able to quantify exposure bias well would also be important for proper quantitative analysis. One promising paper in this direction is <a href="https://arxiv.org/abs/1904.03971" target="_blank" rel="noopener">“Jointly Measuring Diversity and Quality in Text Generation Models” (Montahaei et al., 2019)</a>.</p>
<h2 id="Conclusion">Conclusion</h2>
<p>While recent work has demonstrated an immense improvement in the quality of neural text generation due to the increase in model sizes, the problem of exposure bias still persists for long sequences of generated tokens. Progress in this area will likely require drawing from the work of Reinforcement Learning; indeed, many promising works at this junction of Reinforcement Learning and Language Modelling have already emerged. Hopefully, these improved language models will be competitive with human text not only at the scale of single paragraphs, but potentially entire articles.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>While some may criticize the autoregressive formulation because people generally don’t write purely autoregressively, there actually are authors who use this sort of technique to write <em>entire books</em>.</p>
<blockquote>
<p>While working on each sentence, he has no idea what the next sentence is going to be, much less the next chapter or the end of the book.
<cite><a href="https://web.archive.org/web/20120730190435/http://www.dareland.com/emulsionalproblems/robbins.htm" target="_blank" rel="noopener">Michael Dare on Tom Robbins’s writing style</a></cite></p>
</blockquote>
 <a href="#fnref1" class="footnote-backref">↩︎</a></li>
<li id="fn2" class="footnote-item"><p>An unmodified sample from 117M, generated with a beam-width of 8, top-k of 2, repetition penalty, and conditioned on the unicorn prompt <a href="https://gist.github.com/leogao2/eefe1c1ed512559c20c84f9d797b68e1" target="_blank" rel="noopener">is available here</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>The authors also add an additional “sequence-level” objective that generates sequences from the model and uses repeating ngrams from those sequences to populate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><msup><mi mathvariant="script">C</mi><mi>t</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{C^t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathcal" style="margin-right:0.05834em;">C</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span>. While this does help a bit with exposure bias, the training objective still aims to reduce repetition explicitly. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://leogao.dev/2019/10/27/The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models/" data-id="ckpfzorja0009qtfmu9fmocnz" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPT-2/">GPT-2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Language-Models/">Language Models</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Natural-Language-Processing/">Natural Language Processing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/11/09/Converting-HuggingFace-GPT2-Models-to-Tensorflow-1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Converting HuggingFace GPT-2 Models to Tensorflow 1.x
        
      </div>
    </a>
  
  
    <a href="/2019/09/20/Hello-World/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello, World!</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside class="sidebar sidebar-left">
    <div class="">
        <!--div class="mainnav">
            <p><a href='/' class="mainnav-link">Home</a></p>
            
        </div-->
    </div>
    <div class="leftfnbegin"></div>
    <div class="sidebar-left-footnotes"></div>
</aside>
<aside class="sidebar sidebar-right">
    
      
  <div class="widget-wrap sidebar-align">
    <h3 class="widget-title">About Me</h3>
    <ul class="about-me">
      
      <li><div class="avatar"><img class="ava-img" title="About me" src="/images/profile_small.png" /></div></li>
      
      <div class="authname">Leo Gao</div>
      <div class="sdesc"></div>

      <div class="contact">
          <div class="contact-item"><a href="https://twitter.com/nabla_theta"><i class="fa fa-twitter" aria-hidden="true"></i> @nabla_theta</a></div>
          <div class="contact-item"><a href="https://github.com/leogao2"><i class="fa fa-github" aria-hidden="true"></i> leogao2</a></div>
          <div class="contact-item"><a href="mailto:leogao31@gmail.com"><i class="fa fa-envelope" aria-hidden="true"></i> leogao31@gmail.com</a></div>
      </div>
    </ul>


  </div>
  
    
      
  <div class="widget-wrap sidebar-align">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget recentposts">
      <ul>
        
          <li>
            <a href="/2021/06/02/Thoughts-on-the-Alignment-Implications-of-Scaling-Language-Models/">Thoughts on the Alignment Implications of Scaling Language Models</a>
          </li>
        
          <li>
            <a href="/2020/08/17/Building-AGI-Using-Language-Models/">Building AGI Using Language Models</a>
          </li>
        
          <li>
            <a href="/2020/05/29/GPT-3-A-Brief-Summary/">Why GPT-3 Matters</a>
          </li>
        
          <li>
            <a href="/2020/04/01/Breaking-Down-Reddit-s-Imposter/">Breaking Down Reddit&#39;s Imposter</a>
          </li>
        
          <li>
            <a href="/2020/01/31/Multidisk-Filesystems-A-Comparison/">Multidisk Filesystems: A Comparison</a>
          </li>
        
      </ul>
    </div>
  </div>

    
    <div class="rightfnbegin"></div>
    <div class="sidebar-right-footnotes"></div>

</aside>

<div class="mobile-fn-float-wrapper">
    <div class="mobile-fn-float">
        <div class="footnote-float-numeral footnote-float-numeral-mobile"></div>
        <div class="footnote-float-content"></div>
        <div class="footnote-float-overflow">...</div>
    </div>
</div>
        
      </div>
      <!--
<footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Leo Gao<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
-->
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="https://twitter.com/nabla_theta" class="mobile-nav-link">Twitter</a>
  
    <a href="https://github.com/leogao2" class="mobile-nav-link">Github</a>
  
</nav>
    

<script src="/js/jquery.min.js"></script>
<script src="/js/footnotes.js"></script>
<script src="https://unpkg.com/@popperjs/core@2"></script>
<script src="https://unpkg.com/tippy.js@6"></script>
<script src="/js/acronym-fancifier.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>