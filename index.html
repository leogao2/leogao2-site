<!DOCTYPE html>
<html>

        <!--
            What brings you to these parts, fellow adventurer?
        -->


<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-146499048-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <title>Leo Gao</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Leo Gao">
<meta property="og:url" content="https://leogao.dev/index.html">
<meta property="og:site_name" content="Leo Gao">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Leo Gao">
<meta name="twitter:creator" content="@nabla_theta">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/prism.css" type="text/css"><script src="/js/prism.js"></script><!-- hexo-inject:begin --><link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css'><!-- hexo-inject:end --></head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo"></a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://leogao.dev"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Converting-HuggingFace-GPT2-Models-to-Tensorflow-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <div class="article-date">
  Created: <time datetime="2019-11-09T07:00:00.000Z" itemprop="datePublished">2019-11-09</time>&emsp;
  Modified: <time datetime="2019-11-09T07:00:00.000Z" itemprop="datePublished">2019-11-09</time>
</div>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/09/Converting-HuggingFace-GPT2-Models-to-Tensorflow-1/">Converting HuggingFace GPT2 Models to Tensorflow 1.x</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">HuggingFace Transformers</a> is a wonderful suite of tools for working with transformer models in both Tensorflow 2.x and Pytorch. However, many tools are still written against the original TF 1.x code published by OpenAI. Unfortunately, the model format is different between the TF 2.x models and the original code, which makes it difficult to use models trained on the new code using the old code. There are many tools for converting the old format to TF 2.x and Pytorch, but not vice versa. In this blog post, I will go through the various challenges of backporting the new model format. </p>
<h1 id="I-just-want-to-use-the-final-result"><a href="#I-just-want-to-use-the-final-result" class="headerlink" title="I just want to use the final result"></a>I just want to use the final result</h1><p>The final code is available <a href="https://github.com/leogao2/gpt2-hf-to-tf1" target="_blank" rel="noopener">here</a>.</p>
<h1 id="First-Attempt"><a href="#First-Attempt" class="headerlink" title="First Attempt"></a>First Attempt</h1><p>My first attempt was to use <code>TFGPT2LMHeadModel</code> to load in tensorflow and save a checkpoint immediately using <code>save_pretrained</code>. However, I immediately ran into a problem: <code>save_pretrained</code> saves the result as an HDF5 file, instead of as a TF checkpoint. After some mucking around, I found that the <code>save_pretrained</code> methos called the <code>save_weights</code> method with a fixed <code>tf_model.h5</code> filename, which inferred the save format via the extension- HDF5 for <code>.h5</code> and TF checkpoint otherwise. The solution was just to call <code>save_weights</code> directly, bypassing the filename. For some reason, this wouldn’t save the <code>.meta</code> file containing the graph, but since the graph was the same as in the original OpenAI checkpoints, they could just be copied over. </p>
<p>The model checkpoint contained about the right files, so I put it in the models directory, and.. it didn’t work. </p>
<h1 id="Second-Attempt"><a href="#Second-Attempt" class="headerlink" title="Second Attempt"></a>Second Attempt</h1><p>Apparently, due to the use of keras wrappers (and differently names variables), the exported names were significantly different; <code>model/h0/attn/c_attn/w</code> in the OpenAI model was <code>transformer/h/0/attn/c_attn/weight/.ATTRIBUTES/VARIABLE_VALUE</code> in the huggingface tf model! I found a <a href="https://gist.github.com/batzner/7c24802dd9c5e15870b4b56e22135c96" target="_blank" rel="noopener">script for renaming tf variables in checkpoints</a> and painstakingly combed through the differences in variable names to produce this hodgepodge of replaces:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">new_name = new_name[12:].replace(&apos;/.ATTRIBUTES/VARIABLE_VALUE&apos;, &apos;&apos;)</span><br><span class="line">new_name = new_name.replace(&apos;weight&apos;, &apos;w&apos;)</span><br><span class="line">new_name = new_name.replace(&apos;bias&apos;, &apos;b&apos;)</span><br><span class="line">new_name = new_name.replace(&apos;beta&apos;, &apos;b&apos;)</span><br><span class="line">new_name = new_name.replace(&apos;gamma&apos;, &apos;g&apos;)</span><br><span class="line">if &apos;wpe&apos; in new_name:</span><br><span class="line">    new_name = &apos;wpe&apos;</span><br><span class="line">if &apos;wte&apos; in new_name:</span><br><span class="line">    new_name = &apos;wte&apos;</span><br><span class="line">new_name = &apos;model/&apos; + new_name</span><br><span class="line">new_name = new_name.replace(&apos;/h/&apos;, &apos;/h&apos;)</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://leogao.dev/2019/11/09/Converting-HuggingFace-GPT2-Models-to-Tensorflow-1/" data-id="ck2s0ihml0000r9ml99pwisuv" class="article-share-link">Share</a>
      
        <a href="https://leogao.dev/2019/11/09/Converting-HuggingFace-GPT2-Models-to-Tensorflow-1/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPT2/">GPT2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Language-Models/">Language Models</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <div class="article-date">
  Created: <time datetime="2019-10-27T06:00:00.000Z" itemprop="datePublished">2019-10-27</time>&emsp;
  Modified: <time datetime="2019-10-27T06:00:00.000Z" itemprop="datePublished">2019-10-27</time>
</div>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/27/The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models/">The Difficulties of Text Generation using Autoregressive Language Models: A Brief Overview</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Interest in text-generating models has been rekindled in the past year—in large part due to <a href="https://openai.com/blog/better-language-models/" target="_blank" rel="noopener">GPT2</a>, which primarily demonstrates the effectiveness of using the Transformer architecture with <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" target="_blank" rel="noopener">bigger models, bigger data, and bigger compute</a>. Notably, this model achieved <a href="https://paperswithcode.com/sota/language-modelling-on-wikitext-103" target="_blank" rel="noopener">SOTA results on several language modelling datasets</a> without even training on those datasets, showing its impressive generalization capabilities. Following GPT2, several other entities have also jumped on the bandwagon and released their own large unidirectional language models, such as: <a href="https://rowanzellers.com/grover/" target="_blank" rel="noopener">Grover</a>, <a href="https://nv-adlr.github.io/MegatronLM" target="_blank" rel="noopener">Nvidia’s Megatron-LM</a>, and <a href="https://einstein.ai/presentations/ctrl.pdf" target="_blank" rel="noopener">Salesforce’s CTRL</a>. Setting aside the controversy surrounding OpenAI’s claims that the model is “too dangerous to release,” the text generated by GPT2 are undeniably far better than previous text generation models. However, these models also exhibit some flaws that may not be fixable purely using the bigger-model paradigm. In this post, we take a quick look at some of these flaws and the attempts to solve them, and discuss some potential directions for future research.</p>
<h2 id="What-is-an-autoregressive-language-model-and-why-does-it-matter"><a href="#What-is-an-autoregressive-language-model-and-why-does-it-matter" class="headerlink" title="What is an autoregressive language model and why does it matter?"></a>What is an autoregressive language model and why does it matter?</h2><p>The core problem of language modelling is approximating the distribution of natural language sequences occurring in English (or Lojban, Navajo, Python, etc) using a parameterized function. To make modelling more manageable, the autoregressive language model formulation factors the ideal language model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p^*(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord">∗</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> into:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtable><mtr><mtd><mrow><msup><mi>p</mi><mo>∗</mo></msup><mo>(</mo><mi>x</mi><mo>)</mo><mo>≈</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msub><mover accent="true"><mrow><mi>p</mi></mrow><mo>^</mo></mover><mi>θ</mi></msub><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{aligned}
p^*(x) \approx \prod_{i=1}^{n} \hat{p}_\theta(x_i | x_{&lt;i})
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.714533em;"></span><span class="strut bottom" style="height:2.929066em;vertical-align:-1.214533em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist"><span style="top:-0.06313599999999986em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord">∗</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">≈</span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-0.000005000000000143778em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span><span class="op-symbol large-op mop">∏</span></span></span><span style="top:-1.2500050000000003em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord mathit">p</span></span></span><span style="top:0em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mrel">&lt;</span><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></span></span>

<p>In other words, to make the modelling problem more tractable, we instead train the parameterized function <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mrow><mi>p</mi></mrow><mo>^</mo></mover><mi>θ</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\hat{p}_\theta(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord accent"><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="mord textstyle cramped"><span class="mord mathit">p</span></span></span><span style="top:0em;margin-left:0.16668em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> predict the next token conditioned on the previous tokens, and repeat this using the newly generated tokens, appended to the original context, as the new context. We can then obtain an estimate for the likelihood of any given sequence by taking the product across these conditional probabilities. </p>
<p>Many problems—including classification and translation—can be equivalently formulated as autoregressive problems or would <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">benefit significantly from a strong pretrained language model</a>. Improving language modelling would also potentially be a major step towards solving <a href="http://mattmahoney.net/dc/rationale.html" target="_blank" rel="noopener">the general AI problem</a>. </p>
<h2 id="Beam-search-and-repetition"><a href="#Beam-search-and-repetition" class="headerlink" title="Beam search and repetition"></a>Beam search and repetition</h2><blockquote>
<p>[…] using likelihood as a decoding objective leads to text that is bland and strangely repetitive.<br>—<cite>Holzman et al. 2019</cite></p>
</blockquote>
<p>In the GPT2 samples provided, the authors decided to sample with top-k filtering and temperature rather than with beam search, which would be expected to return much higher-quality samples by maximizing likelihood. It was rather surprising, then, when <a href="https://arxiv.org/abs/1904.09751" target="_blank" rel="noopener">“The Curious Case of Neural Text Degeneration” (Holzman et al. 2019)</a> showed that GPT2 samples with higher predicted likelihood (i.e found via beam search) actually have much lower quality, tending to be extremely repetitive. The authors argue that this modelling problem is due to maximum-likelihood being a fundamentally incorrect sampling objective, and propose nucleus sampling, a sampling method that truncates low-likelihood token predictions (which can lead the model to a “downward spiral”), similar to top-k, while preserving “broad” (tail-heavy) distributions. It could be argued, however, that since sampling a maximum-likelihood sample from the ideal language model <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>x</mi></mrow></msub><msup><mi>p</mi><mo>∗</mo></msup><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">argmax_{x} p^*(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord">∗</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> would, by definition, provide the most likely English text, it would <em>already take into account</em> the unlikelihood of extremely bland and repetitive text in English! Thus, the fault lies with the training objective, not the sampling objective. </p>
<p>Another tempting solution is simply to penalize repetition. In fact, shortly following the publication of the Neural Text Degeneration paper, I independently implemented my own GPT2 beam search sampler; after reproducing the text degeneration issues, I added a simple, arbitrary decoding-time penalty for repeated ngrams, with acceptable results at first glance but little theoretical justification.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> More recently, <a href="https://arxiv.org/abs/1908.04319" target="_blank" rel="noopener">“Neural Text <del>De</del>Generation with Unlikelihood Training” (Welleck, Kulikov et al. 2019)</a> has proposed using a more complex training-time penalization scheme that involves adding a term <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mi>k</mi><msub><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mrow><msup><mi mathvariant="script">C</mi><mi>t</mi></msup></mrow></mrow></msub><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>θ</mi></msub><mo>(</mo><mi>c</mi><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">-k \sum_{c \in \mathcal{C^t}} log(1 - p_\theta(c | x_{&lt;t}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.07738em;vertical-align:-0.32738em;"></span><span class="base textstyle uncramped"><span class="mord">−</span><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span><span class="mrel">∈</span><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathcal" style="margin-right:0.05834em;">C</span><span class="vlist"><span style="top:-0.289em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord"><span class="mord mathit">p</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">c</span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mrel">&lt;</span><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> to the training objective where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><msup><mi mathvariant="script">C</mi><mi>t</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{C^t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathcal" style="margin-right:0.05834em;">C</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span> is a set of previously used tokens.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> While empirically successful, there is no good theoretical reason why less repetition would better model the underlying distribution.</p>
<h2 id="Exposure-Bias"><a href="#Exposure-Bias" class="headerlink" title="Exposure Bias"></a>Exposure Bias</h2><blockquote>
<p>[…] the text will usually fall off a quality cliff after a certain point, suddenly becoming strikingly ungrammatical and typo-ridden and full of anomalous paragraph breaks.<br>—<cite>nostalgebraist</cite></p>
</blockquote>
<p>One major problem with maximum-likelihood training of autoregressive models is <a href="https://arxiv.org/pdf/1511.06732.pdf" target="_blank" rel="noopener">exposure bias (Ranzato et al., 2015)</a>. Autoregressive models are only trained and evaluated on samples drawn from the target language distribution, but at evaluation time are fed samples that are themselves generated by the model. This error compounds extremely quickly and it has been observed, though admittedly anecdotally, that GPT2 exhibits a <a href="https://nostalgebraist.tumblr.com/post/187579086034/it-seems-pretty-clear-to-me-by-now-that-gpt-2-is" target="_blank" rel="noopener">sharp drop-off in quality</a> after a certain number of steps.</p>
<h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><p>This problem bears striking resemblance to many problems in reinforcement learning; indeed, existing works such as <a href="https://arxiv.org/abs/1609.05473" target="_blank" rel="noopener">“SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient” (Yu et al., 2016)</a>, <a href="https://arxiv.org/abs/1808.05599" target="_blank" rel="noopener">“Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation” (Tuan et al., 2018)</a>, and <a href="https://arxiv.org/abs/1804.11258" target="_blank" rel="noopener">“Toward Diverse Text Generation with Inverse Reinforcement Learning” (Shi et al., 2018)</a> (this is not intended to be an exhaustive list by any means) use RL for various components of the training pipeline, from propagating the Generator gradient in a GAN setting to using Inverse Reinforcement Learning (which is itself <a href="https://arxiv.org/abs/1611.03852" target="_blank" rel="noopener">deeply connected to GANs</a>).</p>
<p>There is still a long way to go before these reinforcement learning based options become practical for models as large as the ones in GPT2. An intermediate step is to use existing pretrained language models and tune them in an RL environment. Additionally, an evaluation metric that is able to quantify exposure bias well would also be important for proper quantitative analysis. One promising paper in this direction is <a href="https://arxiv.org/abs/1904.03971" target="_blank" rel="noopener">“Jointly Measuring Diversity and Quality in Text Generation Models” (Montahaei et al., 2019)</a>.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>While recent work has demonstrated an immense improvement in the quality of neural text generation due to the increase in model sizes, the problem of exposure bias still persists for long sequences of generated tokens. Progress in this area will likely require drawing from the work of Reinforcement Learning; indeed, many promising works at this junction of Reinforcement Learning and Language Modelling have already emerged. Hopefully, these improved language models will be competitive with human text not only at the scale of single paragraphs, but potentially entire articles. </p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">An unmodified sample from 117M, generated with a beam-width of 8, top-k of 2, repetition penalty, and conditioned on the unicorn prompt <a href="https://gist.github.com/leogao2/eefe1c1ed512559c20c84f9d797b68e1" target="_blank" rel="noopener">is available here</a></span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">The authors also add an additional “sequence-level” objective that generates sequences from the model and uses repeating ngrams from those sequences to populate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><msup><mi mathvariant="script">C</mi><mi>t</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{C^t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.7935559999999999em;"></span><span class="strut bottom" style="height:0.7935559999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathcal" style="margin-right:0.05834em;">C</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span>. While this does help a bit with exposure bias, the training objective still aims to reduce repetition explicitly.</span><a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://leogao.dev/2019/10/27/The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models/" data-id="ck29vaoi70001bfml99fndwjn" class="article-share-link">Share</a>
      
        <a href="https://leogao.dev/2019/10/27/The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPT2/">GPT2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Language-Models/">Language Models</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Natural-Language-Processing/">Natural Language Processing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Hello-World" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <div class="article-date">
  Created: <time datetime="2019-09-20T06:00:00.000Z" itemprop="datePublished">2019-09-20</time>&emsp;
  Modified: <time datetime="2019-09-20T06:00:00.000Z" itemprop="datePublished">2019-09-20</time>
</div>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/20/Hello-World/">Hello, World!</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Hello, World!</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://leogao.dev/2019/09/20/Hello-World/" data-id="ck29vaoi40000bfmlnsp9hluf" class="article-share-link">Share</a>
      
        <a href="https://leogao.dev/2019/09/20/Hello-World/#disqus_thread" class="article-comment-link">Comments</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">About Me</h3>
    <ul class="about-me">
      
      <li><div class="avatar"><img class="ava-img" title="About me" src="/images/profile.png" /></div></li>
      
      <div class="authname">Leo Gao</div>
      <div class="sdesc">Hello! I am a random internet person who likes to tinker around with software.</div>
    </ul>
    <i class="fa fa-twitter" aria-hidden="true"></i>


  </div>
  
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget recentposts">
      <ul>
        
          <li>
            <a href="/2019/11/09/Converting-HuggingFace-GPT2-Models-to-Tensorflow-1/">Converting HuggingFace GPT2 Models to Tensorflow 1.x</a>
          </li>
        
          <li>
            <a href="/2019/10/27/The-Difficulties-of-Text-Generation-with-Autoregressive-Language-Models/">The Difficulties of Text Generation using Autoregressive Language Models: A Brief Overview</a>
          </li>
        
          <li>
            <a href="/2019/09/20/Hello-World/">Hello, World!</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Leo Gao<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'blogotorium';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>